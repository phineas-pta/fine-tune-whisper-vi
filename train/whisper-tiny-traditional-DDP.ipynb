{
	"cells": [
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# fine-tune whisper tiny with traditional approach + DDP\n",
				"\n",
				"better use of multi GPU with Distributed Data Parallelism (the other notebook use naive model parallelism)\n",
				"\n",
				"**attention**: in this case batch size value is total on all GPU and is evenly splited across (instead of batch size × GPU count)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"collapsed": false,
				"jupyter": {
					"outputs_hidden": false
				},
				"trusted": true
			},
			"outputs": [],
			"source": [
				"from huggingface_hub import notebook_login\n",
				"notebook_login()\n",
				"# !huggingface-cli login --token=███"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"collapsed": false,
				"jupyter": {
					"outputs_hidden": false
				},
				"scrolled": true,
				"trusted": true
			},
			"outputs": [],
			"source": [
				"# workaround for a bug in `datasets` package\n",
				"%pip uninstall -y cudf dask-cuda dask-cudf\n",
				"%pip install -q cudf-cu12 --extra-index-url=https://pypi.nvidia.com\n",
				"%pip install -qU 'datasets[audio]' accelerate transformers jiwer bitsandbytes\n",
				"# install then `import evaluate` throw error on kaggle"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"collapsed": false,
				"jupyter": {
					"outputs_hidden": false
				},
				"trusted": true
			},
			"outputs": [],
			"source": [
				"# everything must be inside this function\n",
				"def train_ddp(pretrained_model, batch_size, total_steps, save_path, resume_training):\n",
				"\timport torch\n",
				"\tfrom dataclasses import dataclass\n",
				"\timport datasets as hugDS\n",
				"\tfrom transformers import WhisperForConditionalGeneration, WhisperFeatureExtractor, WhisperTokenizer, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
				"\tfrom accelerate import Accelerator\n",
				"\timport jiwer\n",
				"\n",
				"\thas_bf16 = torch.cuda.is_bf16_supported()  # GPU Ampere or later\n",
				"\taccelerator = Accelerator(project_dir=save_path, log_with=\"tensorboard\", mixed_precision=\"bf16\" if has_bf16 else \"fp16\")\n",
				"\n",
				"\tFEATURE_EXTRACTOR = WhisperFeatureExtractor.from_pretrained(pretrained_model)\n",
				"\tTOKENIZER = WhisperTokenizer.from_pretrained(pretrained_model, language=\"vi\", task=\"transcribe\")\n",
				"\tMODEL = WhisperForConditionalGeneration.from_pretrained(pretrained_model, use_cache=False, device_map={\"\": accelerator.device})\n",
				"\tMODEL.config.forced_decoder_ids = None\n",
				"\tMODEL.config.suppress_tokens = []\n",
				"\n",
				"\tDUMMY_TOKEN = -100\n",
				"\n",
				"\tSAMPLING_RATE = 16_000\n",
				"\tdef load_my_data(mode, **kwargs):\n",
				"\t\ttmp = hugDS.load_dataset(**kwargs, trust_remote_code=True, streaming=True).cast_column(\"audio\", hugDS.Audio(sampling_rate=SAMPLING_RATE))\n",
				"\t\tmatch mode:\n",
				"\t\t\tcase 0:\n",
				"\t\t\t\treturn tmp\n",
				"\t\t\tcase 1:\n",
				"\t\t\t\treturn tmp.select_columns([\"audio\", \"transcription\"])\n",
				"\t\t\tcase 2:\n",
				"\t\t\t\treturn tmp.select_columns([\"audio\", \"sentence\"]).rename_column(\"sentence\", \"transcription\")\n",
				"\t\t\tcase _:\n",
				"\t\t\t\traise ValueError(\"oh no!\")\n",
				"\n",
				"\n",
				"\twith accelerator.main_process_first():\n",
				"\t\tMY_DATA = hugDS.IterableDatasetDict()\n",
				"\t\tMY_DATA[\"train\"] = hugDS.concatenate_datasets([  # total: 1.5M samples\n",
				"\t\t\tload_my_data(path=\"google/fleurs\",                        name=\"vi_vn\", split=\"train\", mode=1),  # 3k\n",
				"\t\t\tload_my_data(path=\"mozilla-foundation/common_voice_16_1\", name=\"vi\",    split=\"train\", mode=2),  # 2.3k\n",
				"\t\t\tload_my_data(path=\"vivos\",                                              split=\"train\", mode=2),  # 11.7k\n",
				"\t\t\tload_my_data(path=\"doof-ferb/fpt_fosd\",                                 split=\"train\", mode=0),  # 25.9k\n",
				"\t\t\tload_my_data(path=\"doof-ferb/infore1_25hours\",                          split=\"train\", mode=0),  # 14.9k\n",
				"\t\t\tload_my_data(path=\"doof-ferb/vlsp2020_vinai_100h\",                      split=\"train\", mode=0),  # 56.4k\n",
				"\t\t\tload_my_data(path=\"doof-ferb/LSVSC\",                                    split=\"train\", mode=1),  # 45k\n",
				"\t\t\tload_my_data(path=\"quocanh34/viet_vlsp\",                                split=\"train\", mode=0),  # 171k\n",
				"\t\t\tload_my_data(path=\"linhtran92/viet_youtube_asr_corpus_v2\",              split=\"train\", mode=1),  # 195k\n",
				"\t\t\tload_my_data(path=\"doof-ferb/infore2_audiobooks\",                       split=\"train\", mode=0),  # 315k\n",
				"\t\t\tload_my_data(path=\"linhtran92/viet_bud500\",                             split=\"train\", mode=0),  # 634k\n",
				"\t\t])\n",
				"\t\tMY_DATA[\"test\"] = hugDS.concatenate_datasets([  # total: 15k samples\n",
				"\t\t\tload_my_data(path=\"mozilla-foundation/common_voice_16_1\", name=\"vi\", split=\"test\", mode=2),  # 1.3k\n",
				"\t\t\t# remove FLEURS because error when running in batch\n",
				"\t\t\tload_my_data(path=\"vivos\",                                           split=\"test\", mode=2),  # .7k\n",
				"\t\t])\n",
				"\n",
				"\t# some samples will be filtered out later (unknown how many)\n",
				"\n",
				"\tdef prepare_dataset(batch):\n",
				"\t\taudio = batch[\"audio\"]\n",
				"\t\tbatch[\"input_length\"] = len(audio[\"array\"])  # compute input length\n",
				"\t\tbatch[\"input_features\"] = FEATURE_EXTRACTOR(audio[\"array\"], sampling_rate=SAMPLING_RATE).input_features[0]  # compute log-Mel input features\n",
				"\t\tbatch[\"labels\"] = TOKENIZER(batch[\"transcription\"]).input_ids  # encode target text to label ids\n",
				"\t\tbatch[\"labels_length\"] = len(batch[\"labels\"])  # compute labels length\n",
				"\t\treturn batch\n",
				"\n",
				"\tdef filter_inputs(input_length):\n",
				"\t\t\"\"\"Filter inputs with zero input length or longer than 30s\"\"\"\n",
				"\t\treturn 0 < input_length < 48e4  # 30s × 16kHz\n",
				"\n",
				"\tdef filter_labels(labels_length):\n",
				"\t\t\"\"\"Filter label sequences longer than max length 448 tokens\"\"\"\n",
				"\t\treturn labels_length < 448  # MODEL.config.max_length\n",
				"\n",
				"\tMY_DATA = (MY_DATA\n",
				"\t\t# .shuffle(seed=42)  # useless coz streaming multiple datasets (cannot set buffer too high coz not enough RAM)\n",
				"\t\t.map(prepare_dataset)  # no `num_proc` coz streaming\n",
				"\t\t.filter(filter_inputs, input_columns= [\"input_length\"])  # no `remove_columns` coz streaming\n",
				"\t\t.filter(filter_labels, input_columns=[\"labels_length\"])  # no `remove_columns` coz streaming\n",
				"\t)  # TODO: enable `batched=True` but don’t know how to write functions\n",
				"\n",
				"\t@dataclass\n",
				"\tclass DataCollatorSpeechSeq2SeqWithPadding:\n",
				"\t\tdef __call__(self, features):\n",
				"\t\t\t# split inputs and labels since they have to be of different lengths and need different padding methods\n",
				"\t\t\tinput_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
				"\t\t\tlabel_features = [{\"input_ids\"     : feature[\"labels\"]        } for feature in features]  # get the tokenized label sequences\n",
				"\n",
				"\t\t\tbatch = FEATURE_EXTRACTOR.pad(input_features, return_tensors=\"pt\")  # treat the audio inputs by simply returning torch tensors\n",
				"\t\t\tlabels_batch =  TOKENIZER.pad(label_features, return_tensors=\"pt\")  # pad the labels to max length\n",
				"\t\t\tlabels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), DUMMY_TOKEN)  # replace padding with -100 to ignore loss correctly\n",
				"\n",
				"\t\t\tif (labels[:, 0] == TOKENIZER.bos_token_id).all().cpu().item():  # if bos token is appended in previous tokenization step,\n",
				"\t\t\t\tlabels = labels[:, 1:]  # cut bos token here as it’s append later anyways\n",
				"\n",
				"\t\t\tbatch[\"labels\"] = labels\n",
				"\t\t\treturn batch\n",
				"\n",
				"\tDATA_COLLATOR = DataCollatorSpeechSeq2SeqWithPadding()\n",
				"\n",
				"\tJIWER_TRANS = jiwer.Compose([  # DO NOT use `jiwer.RemoveEmptyStrings` it can cause rows count mismatch\n",
				"\t\tjiwer.ToLowerCase(),\n",
				"\t\tjiwer.RemoveKaldiNonWords(),\n",
				"\t\tjiwer.RemoveMultipleSpaces(),\n",
				"\t\tjiwer.Strip(),\n",
				"\t\tjiwer.RemovePunctuation(),\n",
				"\t\tjiwer.ReduceToListOfListOfWords(),\n",
				"\t])\n",
				"\n",
				"\tdef compute_metrics(pred):\n",
				"\t\tpred_ids = pred.predictions\n",
				"\t\tlabel_ids = pred.label_ids\n",
				"\t\tlabel_ids[label_ids == DUMMY_TOKEN] = TOKENIZER.pad_token_id  # replace -100 with the pad_token_id\n",
				"\n",
				"\t\twer = jiwer.wer(  # we do not want to group tokens when computing the metrics\n",
				"\t\t\treference=TOKENIZER.batch_decode(label_ids, skip_special_tokens=True),\n",
				"\t\t\thypothesis=TOKENIZER.batch_decode(pred_ids, skip_special_tokens=True),\n",
				"\t\t\treference_transform=JIWER_TRANS, hypothesis_transform=JIWER_TRANS\n",
				"\t\t)\n",
				"\t\treturn {\"wer\": wer}\n",
				"\n",
				"\tTRAINING_ARGS = Seq2SeqTrainingArguments(\n",
				"\t\toutput_dir=save_path,\n",
				"\t\tper_device_train_batch_size=batch_size,\n",
				"\t\tper_device_eval_batch_size=batch_size,\n",
				"\t\tfp16=not has_bf16,\n",
				"\t\tbf16=has_bf16, tf32=has_bf16,\n",
				"\t\t# torch_compile=True,  # SDPA not support whisper yet\n",
				"\t\treport_to=[\"tensorboard\"],\n",
				"\n",
				"\t\tmax_steps=total_steps,\n",
				"\t\tlogging_steps=25,\n",
				"\t\tsave_steps=50,\n",
				"\t\teval_steps=50,\n",
				"\t\tevaluation_strategy=\"steps\",\n",
				"\t\tsave_total_limit=3,\n",
				"\t\taccelerator_config={\"split_batches\": True},\n",
				"\n",
				"\t\toptim=\"adamw_bnb_8bit\",  # 8-bit AdamW optimizer: lower vram usage than default AdamW\n",
				"\t\tlearning_rate=3.75e-5,\n",
				"\t\twarmup_ratio=.05,  # keep between 5-15%\n",
				"\t\tgradient_accumulation_steps=1 if batch_size >= 8 else 8 // batch_size,\n",
				"\t\tgradient_checkpointing=True,\n",
				"\t\tgradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
				"\t\tpredict_with_generate=True,\n",
				"\t\t# generation_num_beams=5,  # require more VRAM\n",
				"\t\tload_best_model_at_end=True,\n",
				"\t\tmetric_for_best_model=\"wer\",\n",
				"\t\tgreater_is_better=False,  # WER is better when lower\n",
				"\t)\n",
				"\n",
				"\tTRAINER = Seq2SeqTrainer(\n",
				"\t\targs=TRAINING_ARGS,\n",
				"\t\tmodel=MODEL,\n",
				"\t\ttrain_dataset=MY_DATA[\"train\"],\n",
				"\t\teval_dataset=MY_DATA[\"test\"],\n",
				"\t\tdata_collator=DATA_COLLATOR,\n",
				"\t\tcompute_metrics=compute_metrics,\n",
				"\t\ttokenizer=FEATURE_EXTRACTOR,  # not TOKENIZER\n",
				"\t)\n",
				"\n",
				"\tTRAINER.train(resume_from_checkpoint=resume_training)\n",
				"\n",
				"\taccelerator.wait_for_everyone()\n",
				"\tif accelerator.is_main_process:\n",
				"\t\tTRAINER.save_model()"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"collapsed": false,
				"jupyter": {
					"outputs_hidden": false
				},
				"trusted": true
			},
			"outputs": [],
			"source": [
				"from accelerate import notebook_launcher"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"collapsed": false,
				"jupyter": {
					"outputs_hidden": false
				},
				"trusted": true
			},
			"outputs": [],
			"source": [
				"%cd /kaggle/working\n",
				"!rm -rf ./my-whisper-tiny"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"collapsed": false,
				"jupyter": {
					"outputs_hidden": false
				},
				"scrolled": true,
				"trusted": true
			},
			"outputs": [],
			"source": [
				"notebook_launcher(train_ddp, args=(\"openai/whisper-tiny\", 16, 21000, \"./my-whisper-tiny\", False), mixed_precision=\"fp16\", num_nodes=1, num_processes=2)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"collapsed": false,
				"jupyter": {
					"outputs_hidden": false
				},
				"trusted": true
			},
			"outputs": [],
			"source": [
				"!zip -FSr res.zip ./my-whisper-tiny"
			]
		}
	],
	"metadata": {
		"accelerator": "GPU",
		"colab": {
			"gpuType": "T4",
			"private_outputs": true,
			"provenance": []
		},
		"kaggle": {
			"accelerator": "nvidiaTeslaT4",
			"dataSources": [],
			"isGpuEnabled": true,
			"isInternetEnabled": true,
			"language": "python",
			"sourceType": "notebook"
		},
		"kernelspec": {
			"display_name": "Python 3",
			"language": "python",
			"name": "python3"
		},
		"language_info": {
			"name": "python",
			"version": "3.11.7"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 0
}
