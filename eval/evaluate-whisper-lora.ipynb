{
	"cells": [
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# evaluate WER of whisper with PEFT LoRA\n",
				"\n",
				"[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/phineas-pta/fine-tune-whisper-vi/blob/main/eval/evaluate-whisper-lora.ipynb)\n",
				"\n",
				"*kaggle TPU crash when running inference* ~~can be used on kaggle TPU, but do not enable `XLA_USE_BF16` because of AMP (Automatic Mixed Precision)~~\n",
				"\n",
				"try `transformers.pipeline` but error with `torch.autocast`"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"from huggingface_hub import notebook_login\n",
				"notebook_login()\n",
				"# !huggingface-cli login --token=███"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# workaround for a bug in `datasets` package\n",
				"%pip uninstall -y cudf dask-cuda dask-cudf\n",
				"%pip install -q cudf-cu12 --extra-index-url=https://pypi.nvidia.com\n",
				"%pip install -qU 'datasets[audio]' accelerate transformers jiwer bitsandbytes peft\n",
				"# install then `import evaluate` throw error on kaggle"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"import torch\n",
				"# import torch_xla.core.xla_model as xm  # on kaggle TPU\n",
				"from peft import PeftModel, PeftConfig\n",
				"from transformers import WhisperForConditionalGeneration, WhisperFeatureExtractor, WhisperTokenizer\n",
				"import datasets as hugDS\n",
				"import jiwer\n",
				"\n",
				"JIWER_TRANS = jiwer.Compose([  # DO NOT use `jiwer.RemoveEmptyStrings` it can cause rows count mismatch\n",
				"\tjiwer.ToLowerCase(),\n",
				"\tjiwer.RemoveKaldiNonWords(),\n",
				"\tjiwer.RemoveMultipleSpaces(),\n",
				"\tjiwer.Strip(),\n",
				"\tjiwer.RemovePunctuation(),\n",
				"\tjiwer.ReduceToListOfListOfWords(),\n",
				"])"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"SAMPLING_RATE = 16_000\n",
				"def load_my_data(**kwargs):\n",
				"\treturn hugDS.load_dataset(**kwargs, split=\"test\", trust_remote_code=True, streaming=True).cast_column(\"audio\", hugDS.Audio(sampling_rate=SAMPLING_RATE))\n",
				"\n",
				"MY_DATA = hugDS.DatasetDict()\n",
				"MY_DATA[\"commonvoice\"] = load_my_data(path=\"mozilla-foundation/common_voice_16_1\", name=\"vi\",  ).select_columns([\"audio\", \"sentence\"])\n",
				"MY_DATA[\"fleurs\"]      = load_my_data(path=\"google/fleurs\",                        name=\"vi_vn\").select_columns([\"audio\", \"transcription\"]).rename_column(\"transcription\", \"sentence\")\n",
				"MY_DATA[\"vivos\"]       = load_my_data(path=\"vivos\"                                             ).select_columns([\"audio\", \"sentence\"])\n",
				"MY_DATA[\"bud500\"]      = load_my_data(path=\"linhtran92/viet_bud500\"                            ).rename_column(\"transcription\", \"sentence\")\n",
				"MY_DATA[\"lsvsc\"]       = load_my_data(path=\"doof-ferb/LSVSC\"                                   ).select_columns([\"audio\", \"transcription\"]).rename_column(\"transcription\", \"sentence\")\n",
				"# samples count: 1326 + 857 + 760 + 7500 + 5683"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"PEFT_MODEL_ID = \"doof-ferb/whisper-large-peft-lora-vi\"  # @param [\"doof-ferb/whisper-large-peft-lora-vi\", \"daila/whisper-large-v3_LoRA_Common-Vi_WER\", \"daila/whisper-large-v3_LoRA_vi\", \"vikas85/whisper-vlsp-peft\", \"vikas85/whisper-vlsp\", \"vikas85/whisper-fosd-peft\", \"vikas85/whisper-fleurs-peft-vi-2\", \"DuyTa/vi-whisper-medium-Lora\", \"vikas85/whisper-cv-fleur-v6\", \"vikas85/fleurs-vn-peft-v2\", \"Yuhthe/openai-whisper-small-vivos-LORA-colab\"]\n",
				"BASE_MODEL_ID = PeftConfig.from_pretrained(PEFT_MODEL_ID).base_model_name_or_path\n",
				"print(\"adapter to\", BASE_MODEL_ID)\n",
				"\n",
				"# declare task & language in extractor & tokenizer have no effect in inference\n",
				"FEATURE_EXTRACTOR = WhisperFeatureExtractor.from_pretrained(BASE_MODEL_ID)\n",
				"TOKENIZER = WhisperTokenizer.from_pretrained(BASE_MODEL_ID)\n",
				"\n",
				"MODEL = PeftModel.from_pretrained(\n",
				"\tWhisperForConditionalGeneration.from_pretrained(BASE_MODEL_ID, torch_dtype=torch.float16).to(\"cuda\"),  # load_in_8bit make inference super slow\n",
				"\t# WhisperForConditionalGeneration.from_pretrained(BASE_MODEL_ID, torch_dtype=torch.bfloat16).to(xm.xla_device()),  # on kaggle TPU\n",
				"\tPEFT_MODEL_ID\n",
				").merge_and_unload(progressbar=True)  # reduce latency with LoRA\n",
				"\n",
				"# the only way to declare task & language\n",
				"DECODER_ID = torch.tensor(\n",
				"\tTOKENIZER.convert_tokens_to_ids([\"<|startoftranscript|>\", \"<|vi|>\", \"<|transcribe|>\", \"<|notimestamps|>\"]),  # [50258, 50278, 50359, 50363] except for large-v3: [50258, 50278, 50360, 50364]\n",
				"\tdevice=MODEL.device\n",
				").unsqueeze(dim=0)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"@torch.autocast(device_type=\"cuda\")  # required by PEFT\n",
				"# @torch.autocast(device_type=\"xla\", dtype=torch.bfloat16)  # on kaggle TPU\n",
				"@torch.inference_mode()\n",
				"def predict(batch):\n",
				"\tinputs = FEATURE_EXTRACTOR(batch[\"audio\"][\"array\"], sampling_rate=SAMPLING_RATE, return_tensors=\"pt\").to(MODEL.device)\n",
				"\tpredicted_ids = MODEL.generate(input_features=inputs.input_features, decoder_input_ids=DECODER_ID)\n",
				"\tbatch[\"pred\"] = TOKENIZER.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
				"\treturn batch\n",
				"\n",
				"MY_DATA_BIS = MY_DATA.map(predict, remove_columns=[\"audio\"])  # progress bar included"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"for split in MY_DATA_BIS.keys():\n",
				"\twer = 100 * jiwer.wer(\n",
				"\t\treference=MY_DATA_BIS[split][\"sentence\"], hypothesis=MY_DATA_BIS[split][\"pred\"],\n",
				"\t\treference_transform=JIWER_TRANS,          hypothesis_transform=JIWER_TRANS,\n",
				"\t)\n",
				"\tprint(f\"WER on {split} = {wer:.1f}%\")"
			]
		}
	],
	"metadata": {
		"accelerator": "GPU",
		"colab": {
			"gpuType": "T4",
			"private_outputs": true,
			"provenance": []
		},
		"kaggle": {
			"dataSources": [],
			"isGpuEnabled": true,
			"isInternetEnabled": true,
			"language": "python",
			"sourceType": "notebook"
		},
		"kernelspec": {
			"display_name": "Python 3",
			"language": "python",
			"name": "python3"
		},
		"language_info": {
			"name": "python"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 0
}
